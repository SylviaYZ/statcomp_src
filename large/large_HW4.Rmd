---
title: "Homework 4 - Working with large datasets"
author: "your name here"
date: "`r format(Sys.time(), '%m/%d/%Y')`"
output: html_document
---

# Question 1 - benchmark data.table's grouping on real data

Download the merged College Scorecard data from (2009-2016) from here: <https://www.dropbox.com/s/ex0u45rlrjr6h7e/Scorecard_2009-2016.csv?dl=0>

This file is the final merged form of the original data that was discussed in class, using the shell operation. Please use this file for the subsequent questions. Excluding the header, there should be 67,418 rows in this file. 

In class we performed some simple subsetting and grouping
operations. The lecture notes use a previous version of the dataset,
and since they were compiled, `CONTROL` is now integer valued, and
the `TUITFTE` and `SAT_AVG` columns will need to be coerced to a
numeric using `as.numeric`, before you can work with it. (This will
give a warning about NAs introduced, which you should ignore.)

Also you should convert `CONTROL` to a factor, and then change the
levels 1,2,3 to instead `pub`,`pnp`,`pfp`.

From the data dictionary, we have: 

| C | Value              |
|---|--------------------|
| 1 | Public             |
| 2 | Private nonprofit  |
| 3 | Private for-profit |

First, tabulate the number of schools you have in the table for each
value of `CONTROL` (you can use data.table or base R for this). Also
tabulate, the number of schools for each value of `CONTROL` that have
non-NA values for both `TUITFTE` *and* `SAT_AVG`.

Then, compute the mean and SD tuition per FTE and the mean and SD average
SAT for each of the classes of ownership (pub, pnp, pfp), (1) using
data.table, and (2) using `aggregate` with the columns `TUITFTE`,
`SAT_AVG`, `CONTROL` and your NA-removed mean and sd function. Confirm
by eye that they give the same result and compare speed. You can
benchmark with `times=10`.

A typical use of aggregate is:

```
aggregate(df[,c("col1","col2")], df[,"grouping"], function(x) ...)
```

# Question 2- doing more with "by" in data.table

Make a subset of the data, called `scores.sub`, which has complete
data for both `TUITFTE` and `SAT_AVG`. You can look up the `na.omit`
function in data.table.

Make a plot of `SAT_AVG` over `TUITFTE`, and color the points by
`CONTROL`, with x-limits of [0-40,000] and y-limits of [500-1600].

Now tabulate the number of schools that have tuition per FTE over
20,000 and/or average SAT over 1200, grouped by ownership
category. Your output should be sorted on the groupings you define, so
the first row should be public, TUITFTE < 20,000 and SAT_AVG < 1200,
and so on for 12 rows. See the Introduction vignette for data.table
for insight on how to perform this operation. Hint: "sorted by" and
"expressions in by".

# Question 3 - subsets of data 

Use data.table to obtain the tuition per FTE and average SAT for the
two schools with the top average SAT within each ownership
group. Hint: I performed this in two steps, first by ordering
`scores.sub`, and then using "subset of data". Make sure to avoid
returning all of the columns...

# Question 4 - MovieLens sparse dataset

As we mentioned in class, one common form of sparse data is when we
have information about individuals and their interaction with a large
set of items (e.g. movies, products, etc.). The interactions may be
ratings or purchases. One publicly available dataset of movie ratings
is *MovieLens*, which has a 1 MB download available here:

<https://grouplens.org/datasets/movielens/>

Download the `ml-latest-small.zip` dataset. Take a look at each of the
CSV files. How many of the movies have the "Comedy" genre attached to
them? 

Build a sparse matrix of the movies by users, and just put a 1 for if
the user rated the movie (don't actually record the value of the
rating itself). You can do this by specifying `x=1`. In
the abstract, this is a very large matrix, but this is because the
user IDs go up to nearly 200,000. Remove the rows of the sparse matrix
where there are no ratings to produce a sparse matrix that is roughly
~10,000 by ~600. Use `summary` to investigate the range, quartiles,
etc. of number of movies rated by each user.

There are multiple ways to compute the SVD of a sparse matrix. If
after manipulating the matrix in its sparse form, it is not too large
(as in this case), one can just run `svd` which will coerce the matrix
into a dense one. Or there are special functions in packages which are
designed to compute (potentially sparse) SVD solutions on sparse
matrices. Two such functions are `sparsesvd::sparsesvd` and
`irlba::ssvd`. You can choose any of these three methods, in either
case you should specify to return only 3 left singular vectors
(`nu=3`, `rank=3`, or `k=3`, respectively). For `ssvd` in the irlba
package, you should specify that the number of nonzero components
in the right singular vectors should be all (the number of rows of x),
which will give a warning that you should ignore. All of these methods
will produce roughly the same decomposition, with arbitrary sign
changes on the singular vectors. The sparse versions are about 1000
times faster, as they do not coerce the matrix into a dense version.

Compute the SVD of the matrix using one of the methods above. Plot the
columns of the U matrix against each other: 1 vs 2, 2 vs 3, 1
vs 3. Note that column 1 and 3 are correlated, with a long tail of
movies. Investigate the names of these movies. What property can you
infer about the top 6 movies in the tail w.r.t. column 1 and 3? Now
look at the extremes of column 2 of U. What difference can you tell
about the movies, between the smallest values and the largest values
in column 2?

Hint: there are a few movies which are in the `movies.csv` file, but
are not in the `ratings.csv` file. I recommend to subset the list of
movies first, which will help with this problem.
